{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Hari Raval\n",
    "# Course: COS 424\n",
    "# Final Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all csv files\n",
    "game_details_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/games_details.csv\")\n",
    "games_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/games.csv\")\n",
    "players_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/players.csv\")\n",
    "ranking_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/ranking.csv\")\n",
    "teams_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/teams.csv\")\n",
    "# games_df = pd.read_csv(\"/Users/HariRaval/Desktop/COS-424/Final-Project/NBA_Data/game2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the game data and extract the outcome values \n",
    "\n",
    "games_df = games_df.drop([\"GAME_STATUS_TEXT\", \"GAME_DATE_EST\"], axis = 1)\n",
    "games_df = games_df.dropna()\n",
    "game_result = games_df[\"HOME_TEAM_WINS\"]\n",
    "games_df = games_df.drop([\"HOME_TEAM_WINS\"], axis = 1)\n",
    "\n",
    "game_details_df = game_details_df.drop([\"TEAM_ABBREVIATION\", \"TEAM_CITY\", \"PLAYER_NAME\", \"START_POSITION\",\"COMMENT\"], axis = 1)\n",
    "game_details_df = game_details_df.dropna()\n",
    "game_details_result_df = game_details_df[\"PLUS_MINUS\"]\n",
    "game_details_df = game_details_df.drop([\"PLUS_MINUS\"], axis = 1)\n",
    "\n",
    "# convert the minutes to a decimal numeric value\n",
    "min_col = game_details_df[\"MIN\"]\n",
    "updated_min_col = []\n",
    "\n",
    "for minute in min_col:\n",
    "    new_min = minute.replace(\":\", \".\")\n",
    "    new_min = float(new_min)\n",
    "    updated_min_col.append(new_min)\n",
    "\n",
    "game_details_df[\"MIN\"] = updated_min_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the game ID as the index for looking up games \n",
    "games_df = games_df.set_index(\"GAME_ID\", drop = True)\n",
    "games_df.index.name = 'GAME_ID'\n",
    "\n",
    "# save the player id as the index for looking up players\n",
    "game_details_df = game_details_df.set_index(\"PLAYER_ID\", drop = True)\n",
    "game_details_df.index.name = 'PLAYER_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1A: Gaussian Naive Bayes (predicting game outcome)\n",
    "\n",
    "def gaussian_naive_bayes_games_outcome(games_X_train, games_y_train, games_X_test, games_y_test):\n",
    "    # Gaussian naive bayes with no hyper parameter tuning \n",
    "    naive_bayes_gaussian = GaussianNB()\n",
    "    naive_bayes_gaussian_fit = naive_bayes_gaussian.fit(games_X_train, games_y_train)\n",
    "    predicted_values = naive_bayes_gaussian.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"Gaussian Naive Bayes Accuracy (no hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"Gaussian Naive Bayes F1 Score (no hyper tuning): \", f1)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    return (score, naive_bayes_gaussian, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2A: Linear SVM (predicting game outcome)\n",
    "\n",
    "def svm_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test):\n",
    "    # linear svm with no hyper parameter tuning\n",
    "    svm_lin = LinearSVC()\n",
    "    svm_fit = svm_lin.fit(games_X_train, games_y_train)\n",
    "    predicted_values = svm_lin.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"SVM (With Linear Kernel) Accuracy (no hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"SVM (with Linear Kernel) F1 Score (no hyper tuning): \", f1 )\n",
    "\n",
    "    # linear svm with hyper parameter tuning\n",
    "    parameters = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 'loss' : ['hinge','squared_hinge']}\n",
    "    svm_hyper_tuned = GridSearchCV(svm_lin, parameters)\n",
    "    svm_hyper_tuned_fit = svm_hyper_tuned.fit(games_X_train, games_y_train)\n",
    "    predicted_values = svm_hyper_tuned.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"SVM (With Linear Kernel) Accuracy (hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"SVM (with Linear Kernel) F1 Score (hyper tuning): \", f1 )\n",
    "    print(\"Hypertuned alpha value: \", svm_hyper_tuned.best_estimator_.C)\n",
    "    print(\"Hypertuned loss value: \", svm_hyper_tuned.best_estimator_.loss)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    return (score, svm_hyper_tuned, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3A: Logistic Regression (predicting game outcome)\n",
    "\n",
    "def logistic_regression_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test):\n",
    "\n",
    "    # logistic regression with no hyper parameter tuning\n",
    "    logistic_regression = LogisticRegression(solver = 'liblinear')\n",
    "    logistic_regression_fit = logistic_regression.fit(games_X_train, games_y_train)\n",
    "    predicted_values = logistic_regression.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"Logistic Regression Accuracy (no hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"Logistic Regression F1 Score (no hyper tuning): \", f1 )\n",
    "\n",
    "    # logistic regression with hyper parameter tuning\n",
    "    parameters = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "    logistic_regression_hyper_tuned = GridSearchCV(logistic_regression, parameters)\n",
    "    logistic_regression_hyper_tuned_fit = logistic_regression_hyper_tuned.fit(games_X_train, games_y_train)\n",
    "    predicted_values = logistic_regression_hyper_tuned.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"Logistic Regression Accuracy (hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"Logistic Regression F1 Score (hyper tuning): \", f1)\n",
    "    print(\"Hypertuned C value: \", logistic_regression_hyper_tuned.best_estimator_.C)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    return (score, logistic_regression_hyper_tuned, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4A: Random Forest (predicting game outcome)\n",
    "\n",
    "def random_forest_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test):\n",
    "    \n",
    "    # random forest with no hyper parameter tuning\n",
    "    random_forest_classifier = RandomForestClassifier()\n",
    "    random_forest_classifier_fit = random_forest_classifier.fit(games_X_train, games_y_train)\n",
    "    predicted_values = random_forest_classifier.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"Random Forest Accuracy (no hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"Random Forest F1 Score (no hyper tuning): \", f1 )\n",
    "\n",
    "    # random forest with hyper parameter tuning\n",
    "    parameters = {'n_estimators': [100, 150, 200], 'max_depth' : [None, 10, 20, 30] }\n",
    "    random_forest_classifier_hyper_tuned = GridSearchCV(random_forest_classifier, parameters)\n",
    "    random_forest_classifier_hyper_tuned_fit = random_forest_classifier_hyper_tuned.fit(games_X_train, games_y_train)\n",
    "    predicted_values = random_forest_classifier_hyper_tuned.predict(games_X_test)\n",
    "\n",
    "    score = accuracy_score(games_y_test, predicted_values)\n",
    "    # F1 score source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    f1 = f1_score(games_y_test, predicted_values, zero_division = 1, pos_label = 1)\n",
    "    print(\"Random Forest Accuracy (hyper tuning): \", round(score * 100, 4), \"%\")\n",
    "    print(\"Random Forest F1 Score (hyper tuning): \", f1)\n",
    "    print(\"Hypertuned n_estimators value: \", random_forest_classifier_hyper_tuned.best_estimator_.n_estimators)\n",
    "    print(\"Hypertuned max_depth value: \", random_forest_classifier_hyper_tuned.best_estimator_.max_depth)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    return (score,random_forest_classifier_hyper_tuned, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1B: Plain Linear Regression (predicting plus/minus outcome)\n",
    "\n",
    "def linear_regression_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test):\n",
    "    linear_reg = linear_model.LinearRegression()\n",
    "    linear_reg.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = linear_reg.predict(game_details_X_test)\n",
    "\n",
    "    print(\"Linear Regression R-Squared Value (no hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Linear Regerssion Mean Squared Error Loss (no hyper tuning): \",mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Linear Regression Mean Absolute Error: (no hyper tuning) \",mean_absolute_error(game_details_y_test, predictions))\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    error = mean_absolute_error(game_details_y_test, predictions)\n",
    "    \n",
    "    return (error, linear_reg, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2B: Lasso Regression (predicting plus/minus outcome)\n",
    "\n",
    "def lasso_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test):\n",
    "    # lasso regression with no hyper tuning\n",
    "    lasso = linear_model.Lasso()\n",
    "    lasso.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = lasso.predict(game_details_X_test)\n",
    "    print(\"Lasso R-Squared Value (no hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Lasso Mean Squared Error Loss (no hyper tuning): \",mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Lasoo Mean Absolute Error: (no hyper tuning) \",mean_absolute_error(game_details_y_test, predictions)) \n",
    "\n",
    "    # lasso regression with hyper tuning\n",
    "    parameters = {'alpha': [0.0001,0.001,0.01,0.1,1]}\n",
    "    lasso_hyper_tuned = GridSearchCV(lasso, parameters)\n",
    "    lasso_hyper_tuned.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = lasso_hyper_tuned.predict(game_details_X_test)\n",
    "\n",
    "    print(\"Lasso R-Squared Value (hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Lasso Mean Squared Error Loss (hyper tuning): \",mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Lasso Mean Absolute Error: (hyper tuning) \",mean_absolute_error(game_details_y_test, predictions)) \n",
    "    print(\"Lasso Hypertuned alpha value for regularization: \", lasso_hyper_tuned.best_estimator_.alpha)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    error = mean_absolute_error(game_details_y_test, predictions)\n",
    "    \n",
    "    return (error, lasso_hyper_tuned, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3B: Ridge Regression (predicting plus/minus outcome)\n",
    "\n",
    "def ridge_plus_minus(game_details_X_train,game_details_y_train, game_details_X_test, game_details_y_test):\n",
    "    # ridge regression with no hyper tuning\n",
    "    ridge = linear_model.Ridge()\n",
    "    ridge.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = ridge.predict(game_details_X_test)\n",
    "    print(\"Ridge R-Squared Value (no hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Ridge Mean Squared Error Loss (no hyper tuning): \",mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Ridge Mean Absolute Error (no hyper tuning): \",mean_absolute_error(game_details_y_test, predictions)) \n",
    "\n",
    "    # ridge regression with hyper tuning\n",
    "    parameters = {'alpha': [0.00001,0.0001,0.001,0.01,0.1,1.0]}\n",
    "    ridge_hyper_tuned = GridSearchCV(ridge, parameters)\n",
    "    ridge_hyper_tuned.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = ridge_hyper_tuned.predict(game_details_X_test)\n",
    "\n",
    "    print(\"Ridge R-Squared Value (hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Ridge Mean Squared Error Loss (hyper tuning): \", mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Ridge Mean Absolute Error: (hyper tuning) \", mean_absolute_error(game_details_y_test, predictions)) \n",
    "    print(\"Ridge Hypertuned alpha value for regularization: \", ridge_hyper_tuned.best_estimator_.alpha)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    error = mean_absolute_error(game_details_y_test, predictions)\n",
    "    \n",
    "    return (error, ridge_hyper_tuned, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4B: Elastic Net (predicting plus/minus outcome)\n",
    "\n",
    "def elastic_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test):\n",
    "    # elastic net regression with no hyper tuning\n",
    "    elastic = linear_model.ElasticNet()\n",
    "    elastic.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = elastic.predict(game_details_X_test)\n",
    "    print(\"Elastic R-Squared Value (no hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Elastic Mean Squared Error Loss (no hyper tuning): \",mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Elastic Mean Absolute Error (no hyper tuning): \",mean_absolute_error(game_details_y_test, predictions)) \n",
    "\n",
    "    # elastic net regression with hyper tuning\n",
    "    parameters = {'alpha': [0.0001,0.001,0.01,0.1,1], 'l1_ratio': [0.1,0.25,0.5,0.75,0.9]} # avoid l1_ratio of 0 or 1 since this is same as ridge or lasso\n",
    "    elastic_hyper_tuned = GridSearchCV(elastic, parameters)\n",
    "    elastic_hyper_tuned.fit(game_details_X_train, game_details_y_train)\n",
    "    predictions = elastic_hyper_tuned.predict(game_details_X_test)\n",
    "    print(\"Elastic R-Squared Value (hyper tuning): \", r2_score(game_details_y_test, predictions))\n",
    "    print(\"Elastic Mean Squared Error Loss (hyper tuning): \", mean_squared_error(game_details_y_test, predictions))\n",
    "    print(\"Elastic Mean Absolute Error: (hyper tuning) \", mean_absolute_error(game_details_y_test, predictions)) \n",
    "    print(\"Elastic Hypertuned alpha value for regularization: \", elastic_hyper_tuned.best_estimator_.alpha)\n",
    "    print(\"Elastic Hypertuned l1 ratio value for regularization: \", elastic_hyper_tuned.best_estimator_.l1_ratio)\n",
    "    print(\"-------------------------------------\")\n",
    "    \n",
    "    error = mean_absolute_error(game_details_y_test, predictions)\n",
    "    \n",
    "    return (error, elastic_hyper_tuned, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bootstrapping and computes the confidence interval for three performance measures for regression methods\n",
    "def perform_bootstrapping_regression(number_bootstraps, game_details_df, game_detail_results_df, model):\n",
    "    number_train_samples = int(len(game_details_df) * 0.60)\n",
    "    number_test_samples = int(len(game_details_df) * 0.40)\n",
    "    # lists to store computed statistics\n",
    "    r_squared_list = []\n",
    "    mean_squared_error_list = []\n",
    "    mean_absolute_error_list = []\n",
    "    # fit model and compute performance statistics per bootstrap sample\n",
    "    # Code adapted from: https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "    for i in range(number_bootstraps):\n",
    "        print(\"Bootstrap Sample: \", i)\n",
    "        # create the bootstrap and split the data with resampling  \n",
    "        boot_train_x, boot_train_y = resample(game_details_df, game_detail_results_df, n_samples = number_train_samples)\n",
    "        boot_train_chalg_ids = boot_train_x.index.values.tolist()\n",
    "\n",
    "        current_test_dataframe = game_details_df[~game_details_df.index.isin(boot_train_chalg_ids)]\n",
    "        current_test_responses = game_detail_results_df[~game_detail_results_df.index.isin(boot_train_chalg_ids)]    \n",
    "        boot_test_x, boot_test_y = resample(current_test_dataframe, current_test_responses, n_samples = number_test_samples)\n",
    "\n",
    "        # fit and predict on the current iteration bootstrap        \n",
    "        model.fit(boot_train_x, boot_train_y)\n",
    "        predictions = model.predict(boot_test_x)\n",
    "    \n",
    "        # compute and store the appropriate performance statistics \n",
    "        r_squared_list.append(r2_score(boot_test_y, predictions))\n",
    "        mean_squared_error_list.append(mean_squared_error(boot_test_y, predictions))\n",
    "        mean_absolute_error_list.append(mean_absolute_error(boot_test_y, predictions))\n",
    "        \n",
    "    r_squared_list.sort()\n",
    "    mean_squared_error_list.sort()\n",
    "    mean_absolute_error_list.sort()\n",
    "    \n",
    "    return (r_squared_list, mean_squared_error_list, mean_absolute_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bootstrapping and computes the confidence interval for three performance measures for classification methods\n",
    "def perform_bootstrapping_classification(number_bootstraps, game_details_df, game_detail_results_df, model):\n",
    "    \n",
    "    number_train_samples = int(len(game_details_df) * 0.60)\n",
    "    number_test_samples = int(len(game_details_df) * 0.40)\n",
    "    # lists to store computed statistics\n",
    "    accuracy = []\n",
    "    f_scores = []\n",
    "    \n",
    "    # fit model and compute performance statistics per bootstrap sample\n",
    "    # Code adapted from: https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "    for i in range(number_bootstraps):\n",
    "        print(\"Bootstrap Sample: \", i)\n",
    "        # create the bootstrap and split the data with resampling  \n",
    "        boot_train_x, boot_train_y = resample(game_details_df, game_detail_results_df, n_samples = number_train_samples)\n",
    "        boot_train_chalg_ids = boot_train_x.index.values.tolist()\n",
    "        current_test_dataframe = game_details_df.drop(index = boot_train_chalg_ids)\n",
    "        current_test_responses = game_detail_results_df.drop(index = boot_train_chalg_ids)\n",
    "        boot_test_x, boot_test_y = resample(current_test_dataframe, current_test_responses, n_samples = number_test_samples)\n",
    "\n",
    "        # fit and predict on the current iteration bootstrap\n",
    "        model.fit(boot_train_x, boot_train_y)\n",
    "        predictions = model.predict(boot_test_x)\n",
    "\n",
    "        # compute and store the appropriate performance statistics \n",
    "        accuracy.append(accuracy_score(boot_test_y, predictions)) \n",
    "        f_scores.append(f1_score(boot_test_y, predictions, zero_division = 1, pos_label = 1))\n",
    "        \n",
    "    accuracy.sort()\n",
    "    f_scores.sort()\n",
    "    \n",
    "    return (accuracy, f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confidence interval of the provided measurements \n",
    "def compute_confidence_interval(alpha, data):\n",
    "    # code below from https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = max(0.0, np.percentile(data, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(data, p))\n",
    "    return(lower, upper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the optimal k value for the games dataframe:\n",
    "\n",
    "k_vals = list(range(1,18,2))\n",
    "\n",
    "naive_bayes_scores = []\n",
    "svm_scores = []\n",
    "log_regression_scores = []\n",
    "rand_forest_scores = []\n",
    "\n",
    "# iterate over all k values and select k features, hypertune the model, and save the accuracies per model\n",
    "for curr_k in k_vals:\n",
    "    \n",
    "    games_X_train, games_X_test, games_y_train, games_y_test = train_test_split(games_df, game_result, random_state=42)\n",
    "    \n",
    "    chi2_features = SelectKBest(chi2, k = curr_k)\n",
    "    df_kbest_features = chi2_features.fit_transform(games_X_train, games_y_train)\n",
    "    f = chi2_features.get_support(indices=True)\n",
    "    games_X_train = games_X_train[games_X_train.columns[f]]\n",
    "\n",
    "    test_kbest_features = chi2_features.transform(games_X_test)\n",
    "    f = chi2_features.get_support(indices=True)\n",
    "    games_X_test = games_X_test[games_X_test.columns[f]]\n",
    "\n",
    "    score1 = gaussian_naive_bayes_games_outcome(games_X_train, games_y_train, games_X_test, games_y_test)[0]\n",
    "    naive_bayes_scores.append(score1)\n",
    "    \n",
    "    score2 = svm_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)[0] \n",
    "    svm_scores.append(score2)\n",
    "    \n",
    "    score3 = logistic_regression_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)[0]\n",
    "    log_regression_scores.append(score3)\n",
    "    \n",
    "    score4 = random_forest_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)[0]\n",
    "    rand_forest_scores.append(score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the accuracies for each possible k for the games dataframe\n",
    "\n",
    "column_names = [\"Number of Features\", \"Naive Bayes Acc.\", \"Linear SVM Acc.\", \"Logistic Regression Acc.\", \"Random Forest Acc.\"]\n",
    "\n",
    "accuracies_per_k = pd.DataFrame(columns = column_names)\n",
    "\n",
    "accuracies_per_k[\"Number of Features\"] = k_vals\n",
    "accuracies_per_k[\"Naive Bayes Acc.\"] = naive_bayes_scores\n",
    "accuracies_per_k[\"Linear SVM Acc.\"] = svm_scores\n",
    "accuracies_per_k[\"Logistic Regression Acc.\"] = log_regression_scores\n",
    "accuracies_per_k[\"Random Forest Acc.\"] = rand_forest_scores\n",
    "\n",
    "accuracies_per_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the optimal k value for the game details dataframe:\n",
    "\n",
    "k_vals = list(range(1,22,2))\n",
    "\n",
    "linear_regression_errors = []\n",
    "ridge_errors = []\n",
    "lasso_errors = []\n",
    "elastic_errors = []\n",
    "\n",
    "# iterate over all k values and select k features, hypertune the model, and save the accuracies per model\n",
    "for curr_k in k_vals:\n",
    "    \n",
    "    game_details_X_train, game_details_X_test, game_details_y_train, game_details_y_test = train_test_split(game_details_df, game_details_result_df, random_state=42)\n",
    "    \n",
    "    chi2_features = SelectKBest(chi2, k = curr_k)\n",
    "    df_kbest_features = chi2_features.fit_transform(game_details_X_train, game_details_y_train)\n",
    "    f = chi2_features.get_support(indices=True)\n",
    "    game_details_X_train = game_details_X_train[game_details_X_train.columns[f]]\n",
    "\n",
    "    test_kbest_features = chi2_features.transform(game_details_X_test)\n",
    "    f = chi2_features.get_support(indices=True)\n",
    "    game_details_X_test = game_details_X_test[game_details_X_test.columns[f]]\n",
    "\n",
    "    score1 = linear_regression_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)[0]\n",
    "    linear_regression_errors.append(score1)\n",
    "    \n",
    "    score2 = lasso_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)[0]\n",
    "    lasso_errors.append(score2)\n",
    "    \n",
    "    score3 = ridge_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)[0]\n",
    "    ridge_errors.append(score3)\n",
    "    \n",
    "    score4 = elastic_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)[0]\n",
    "    elastic_errors.append(score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the absolute errors for each possible k for the game details for the dataframe\n",
    "\n",
    "column_names = [\"Number of Features\", \"Linear Regression Errors\", \"Lasso Errors\", \"Ridge Errors\", \"Elastic Net Errors\"]\n",
    "\n",
    "errors_per_k = pd.DataFrame(columns = column_names)\n",
    "\n",
    "errors_per_k[\"Number of Features\"] = k_vals\n",
    "errors_per_k[\"Linear Regression Errors\"] = linear_regression_errors\n",
    "errors_per_k[\"Lasso Errors\"] = lasso_errors\n",
    "errors_per_k[\"Ridge Errors\"] = ridge_errors\n",
    "errors_per_k[\"Elastic Net Errors\"] = elastic_errors\n",
    "\n",
    "errors_per_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data frames that use the optimal number of features\n",
    "\n",
    "games_X_train, games_X_test, games_y_train, games_y_test = train_test_split(games_df, game_result, random_state=42)\n",
    "game_details_X_train, game_details_X_test, game_details_y_train, game_details_y_test = train_test_split(game_details_df, game_details_result_df, random_state=42)\n",
    "\n",
    "# Perform feature selection on the games dataframe for training and testing \n",
    "chi2_features = SelectKBest(chi2, k = 12)\n",
    "df_kbest_features = chi2_features.fit_transform(games_X_train, games_y_train)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "games_X_train = games_X_train[games_X_train.columns[f]]\n",
    "\n",
    "test_kbest_features = chi2_features.transform(games_X_test)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "games_X_test = games_X_test[games_X_test.columns[f]]\n",
    "\n",
    "# Perform feature selection on the entire games dataset\n",
    "\n",
    "test_kbest_features = chi2_features.transform(games_df)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "games_df = games_df[games_df.columns[f]]\n",
    "\n",
    "# Perform feature selection on the game details dataframe for training and testing\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#game_details_X_train = pd.DataFrame(scaler.fit_transform(game_details_X_train),index = game_details_X_train.index,columns = game_details_X_train.columns)\n",
    "#game_details_X_test = pd.DataFrame(scaler.fit_transform(game_details_X_test),index = game_details_X_test.index,columns = game_details_X_test.columns)\n",
    "#game_details_y_train = game_details_y_train.astype('float64')\n",
    "\n",
    "chi2_features = SelectKBest(chi2, k = 15) \n",
    "df_kbest_features = chi2_features.fit_transform(game_details_X_train, game_details_y_train)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "game_details_X_train = game_details_X_train[game_details_X_train.columns[f]]\n",
    "\n",
    "test_kbest_features = chi2_features.transform(game_details_X_test)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "game_details_X_test = game_details_X_test[game_details_X_test.columns[f]]\n",
    "\n",
    "# Perform feature selection on the entire game details dataframe for use in bootstrapping\n",
    "\n",
    "test_kbest_features = chi2_features.transform(game_details_df)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "game_details_df = game_details_df[game_details_df.columns[f]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data normalization before running final models \n",
    "\n",
    "scaler = StandardScaler()\n",
    "games_X_train = pd.DataFrame(scaler.fit_transform(games_X_train),index = games_X_train.index,columns = games_X_train.columns)\n",
    "games_X_test = pd.DataFrame(scaler.fit_transform(games_X_test),index = games_X_test.index,columns = games_X_test.columns)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "game_details_X_train = pd.DataFrame(scaler.fit_transform(game_details_X_train),index = game_details_X_train.index,columns = game_details_X_train.columns)\n",
    "game_details_X_test = pd.DataFrame(scaler.fit_transform(game_details_X_test),index = game_details_X_test.index,columns = game_details_X_test.columns)\n",
    "\n",
    "# drop the GAME ID column since it is no longer needed \n",
    "game_details_X_train = game_details_X_train.drop(\"GAME_ID\", axis = 1)\n",
    "game_details_X_test = game_details_X_test.drop(\"GAME_ID\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all model results for predicting game outcomes (with normalized data)\n",
    "\n",
    "gaussian_game_outcome = gaussian_naive_bayes_games_outcome(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "\n",
    "svm_game_outcome = svm_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "\n",
    "logistic_reg_game_outcome = logistic_regression_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "\n",
    "random_forest_game_outcome = random_forest_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bootstrapping to build confidence intervals for the classification task\n",
    "\n",
    "game_result = pd.DataFrame(game_result)\n",
    "game_result[\"idx\"] = games_df.index.values\n",
    "game_result = game_result.set_index(\"idx\")\n",
    "\n",
    "# gaussian naive bayes boot strapping (game outcome prediction)\n",
    "accuracy, f1score = perform_bootstrapping_classification(150, games_df, game_result[\"HOME_TEAM_WINS\"], gaussian_game_outcome[1])\n",
    "accuracy_l, accuracy_2 = compute_confidence_interval(0.95, accuracy) \n",
    "f1_score_1, f1_score_2 = compute_confidence_interval(0.95, f1score) \n",
    "print(\"95%% Confidence Interval for naive bayes accuracy Value: [%.4f, %.4f] \" % (accuracy_l, accuracy_2))\n",
    "print(\"95%% Confidence Interval for naive bayes F1 score: [%.4f, %.4f] \" % (f1_score_1, f1_score_2))\n",
    "\n",
    "# linear svm boot strapping (game outcome prediction)\n",
    "accuracy, f1score = perform_bootstrapping_classification(150, games_df, game_result[\"HOME_TEAM_WINS\"], svm_game_outcome[1])\n",
    "accuracy_l, accuracy_2 = compute_confidence_interval(0.95, accuracy) \n",
    "f1_score_1, f1_score_2 = compute_confidence_interval(0.95, f1score) \n",
    "print(\"95%% Confidence Interval for linear svm accuracy Value: [%.4f, %.4f] \" % (accuracy_l, accuracy_2))\n",
    "print(\"95%% Confidence Interval for linear svm F1 score: [%.4f, %.4f] \" % (f1_score_1, f1_score_2))\n",
    "\n",
    "# logistic regression boot strapping (game outcome prediction)\n",
    "accuracy, f1score = perform_bootstrapping_classification(150, games_df, game_result[\"HOME_TEAM_WINS\"], logistic_reg_game_outcome[1])\n",
    "accuracy_l, accuracy_2 = compute_confidence_interval(0.95, accuracy) \n",
    "f1_score_1, f1_score_2 = compute_confidence_interval(0.95, f1score) \n",
    "print(\"95%% Confidence Interval for logistic regression accuracy Value: [%.4f, %.4f] \" % (accuracy_l, accuracy_2))\n",
    "print(\"95%% Confidence Interval for logistic regression F1 score: [%.4f, %.4f] \" % (f1_score_1, f1_score_2))\n",
    "\n",
    "# random forest boot strapping (game outcome prediction)\n",
    "accuracy, f1score = perform_bootstrapping_classification(150, games_df, game_result[\"HOME_TEAM_WINS\"], random_forest_game_outcome[1])\n",
    "accuracy_l, accuracy_2 = compute_confidence_interval(0.95, accuracy) \n",
    "f1_score_1, f1_score_2 = compute_confidence_interval(0.95, f1score) \n",
    "print(\"95%% Confidence Interval for random forest accuracy Value: [%.4f, %.4f] \" % (accuracy_l, accuracy_2))\n",
    "print(\"95%% Confidence Interval for random forest F1 score: [%.4f, %.4f] \" % (f1_score_1, f1_score_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all model results for predicting plus minus outcomes (with normalized data)\n",
    "\n",
    "linear_plus_minus_outcome = linear_regression_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)\n",
    "\n",
    "lasso_plus_minus_outcome = lasso_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)\n",
    "\n",
    "ridge_reg_plus_minus_outcome = ridge_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)\n",
    "\n",
    "elastic_forest_plus_minus_outcome = elastic_plus_minus(game_details_X_train, game_details_y_train, game_details_X_test, game_details_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bootstrapping to build confidence intervals for the regression task\n",
    "\n",
    "game_details_result_df = pd.DataFrame(game_details_result_df)\n",
    "game_details_result_df[\"idx\"] = game_details_df.index.values\n",
    "game_details_result_df = game_details_result_df.set_index(\"idx\")\n",
    "# gaussian naive bayes boot strapping (game outcome prediction)\n",
    "r_squared, mean_squared, mean_absolute = perform_bootstrapping_regression(150, game_details_df, game_details_result_df[\"PLUS_MINUS\"], linear_plus_minus_outcome[1])\n",
    "r_squared_1, r_squared_2 = compute_confidence_interval(0.95, r_squared) \n",
    "mean_squared_1, mean_squared_2 = compute_confidence_interval(0.95, mean_squared)\n",
    "mean_absolute_1, mean_absolute_2 = compute_confidence_interval(0.95, mean_absolute) \n",
    "print(\"95%% Confidence Interval for least squares r squared Value: [%.4f, %.4f] \" % (r_squared_1, r_squared_2))\n",
    "print(\"95%% Confidence Interval for least squares mean squared error: [%.4f, %.4f] \" % (mean_squared_1, mean_squared_2))\n",
    "print(\"95%% Confidence Interval for least squares mean absolute error: [%.4f, %.4f] \" % (mean_absolute_1, mean_absolute_2))\n",
    "\n",
    "# linear svm boot strapping (game outcome prediction)\n",
    "r_squared, mean_squared, mean_absolute = perform_bootstrapping_regression(150, game_details_df, game_details_result_df[\"PLUS_MINUS\"], lasso_plus_minus_outcome[1])\n",
    "r_squared_1, r_squared_2 = compute_confidence_interval(0.95, r_squared) \n",
    "mean_squared_1, mean_squared_2 = compute_confidence_interval(0.95, mean_squared)\n",
    "mean_absolute_1, mean_absolute_2 = compute_confidence_interval(0.95, mean_absolute) \n",
    "print(\"95%% Confidence Interval for lasso r squared Value: [%.4f, %.4f] \" % (r_squared_1, r_squared_2))\n",
    "print(\"95%% Confidence Interval for lasso mean squared error: [%.4f, %.4f] \" % (mean_squared_1, mean_squared_2))\n",
    "print(\"95%% Confidence Interval for lasso mean absolute error: [%.4f, %.4f] \" % (mean_absolute_1, mean_absolute_2))\n",
    "\n",
    "# logistic regression boot strapping (game outcome prediction)\n",
    "r_squared, mean_squared, mean_absolute = perform_bootstrapping_regression(150, game_details_df, game_details_result_df[\"PLUS_MINUS\"], ridge_reg_plus_minus_outcome[1])\n",
    "r_squared_1, r_squared_2 = compute_confidence_interval(0.95, r_squared) \n",
    "mean_squared_1, mean_squared_2 = compute_confidence_interval(0.95, mean_squared)\n",
    "mean_absolute_1, mean_absolute_2 = compute_confidence_interval(0.95, mean_absolute) \n",
    "print(\"95%% Confidence Interval for ridge r squared Value: [%.4f, %.4f] \" % (r_squared_1, r_squared_2))\n",
    "print(\"95%% Confidence Interval for ridge mean squared error: [%.4f, %.4f] \" % (mean_squared_1, mean_squared_2))\n",
    "print(\"95%% Confidence Interval for ridge mean absolute error: [%.4f, %.4f] \" % (mean_absolute_1, mean_absolute_2))\n",
    "\n",
    "# random forest boot strapping (game outcome prediction)\n",
    "r_squared, mean_squared, mean_absolute = perform_bootstrapping_regression(150, game_details_df, game_details_result_df[\"PLUS_MINUS\"], elastic_forest_plus_minus_outcome[1])\n",
    "r_squared_1, r_squared_2 = compute_confidence_interval(0.95, r_squared) \n",
    "mean_squared_1, mean_squared_2 = compute_confidence_interval(0.95, mean_squared)\n",
    "mean_absolute_1, mean_absolute_2 = compute_confidence_interval(0.95, mean_absolute) \n",
    "print(\"95%% Confidence Interval for elastic net r squared Value: [%.4f, %.4f] \" % (r_squared_1, r_squared_2))\n",
    "print(\"95%% Confidence Interval for elastic net mean squared error: [%.4f, %.4f] \" % (mean_squared_1, mean_squared_2))\n",
    "print(\"95%% Confidence Interval for elastic net mean absolute error: [%.4f, %.4f] \" % (mean_absolute_1, mean_absolute_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which features are most important for predicting game outcome per model\n",
    "\n",
    "# extract/find important features for naive bayes\n",
    "\n",
    "   # not trivial/necessary to do -> according to Ed post\n",
    "\n",
    "# extract/find important features for linear svm\n",
    "[importances] = abs(svm_game_outcome[1].best_estimator_.coef_)\n",
    "features = games_X_train.columns\n",
    "svm_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "svm_feature_importance_df = pd.DataFrame(data = svm_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(svm_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# extract/find important features for logistic regression \n",
    "[importances] = abs(logistic_reg_game_outcome[1].best_estimator_.coef_)\n",
    "features = games_X_train.columns\n",
    "log_reg_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "log_reg_feature_importance_df = pd.DataFrame(data = log_reg_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(log_reg_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# extract/find important features for random forest\n",
    "importances = random_forest_game_outcome[1].best_estimator_.feature_importances_\n",
    "features = games_X_train.columns\n",
    "random_forest_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "random_forest_feature_importance_df = pd.DataFrame(data = random_forest_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(random_forest_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which features are most important for predicting player plus minus statistic per model \n",
    "\n",
    "# extract/find important features for plain linear regression \n",
    "importances = abs(linear_plus_minus_outcome[1].coef_)\n",
    "features = game_details_df.columns[1:]\n",
    "linear_reg_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "linear_reg_feature_importance_df = pd.DataFrame(data = linear_reg_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(linear_reg_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# extract/find important features for lasso regression \n",
    "importances = abs(lasso_plus_minus_outcome[1].best_estimator_.coef_)\n",
    "features = game_details_df.columns[1:]\n",
    "lasso_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "lasso_feature_importance_df = pd.DataFrame(data = lasso_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(lasso_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# extract/find important features for ridge regression \n",
    "importances = abs(ridge_reg_plus_minus_outcome[1].best_estimator_.coef_)\n",
    "features = game_details_df.columns[1:]\n",
    "ridge_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "ridge_feature_importance_df = pd.DataFrame(data = ridge_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(ridge_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# extract/find important features for elastic net\n",
    "importances = (elastic_forest_plus_minus_outcome[1].best_estimator_.coef_)\n",
    "features = game_details_df.columns[1:]\n",
    "elastic_net_features_and_importances = {'Top features':features, 'importances':importances}\n",
    "elastic_net_feature_importance_df = pd.DataFrame(data = elastic_net_features_and_importances).sort_values(\"importances\", ascending = False)\n",
    "# output top 5 importances \n",
    "print(elastic_net_feature_importance_df.head(10))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the points scored by home and away teams to see the impact on game prediction\n",
    "\n",
    "# drop the points scored columns\n",
    "\n",
    "games_df = games_df.drop(['PTS_home', 'PTS_away'], axis=1)\n",
    "\n",
    "# split the data after dropping the points scored columns\n",
    "games_X_train, games_X_test, games_y_train, games_y_test = train_test_split(games_df, game_result, random_state=42)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "games_X_train = pd.DataFrame(scaler.fit_transform(games_X_train),index = games_X_train.index,columns = games_X_train.columns)\n",
    "games_X_test = pd.DataFrame(scaler.fit_transform(games_X_test),index = games_X_test.index,columns = games_X_test.columns)\n",
    "\n",
    "# Perform feature selection on the games dataframe for training and testing \n",
    "chi2_features = SelectKBest(chi2, k = 10)\n",
    "df_kbest_features = chi2_features.fit_transform(games_X_train, games_y_train)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "games_X_train = games_X_train[games_X_train.columns[f]]\n",
    "\n",
    "test_kbest_features = chi2_features.transform(games_X_test)\n",
    "f = chi2_features.get_support(indices=True)\n",
    "games_X_test = games_X_test[games_X_test.columns[f]]\n",
    "\n",
    "# run all four models \n",
    "gaussian_game_outcomes = gaussian_naive_bayes_games_outcome(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "svm_lin_game_outcomes = svm_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "logistic_reg_game_outcomes = logistic_regression_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n",
    "random_forest_game_outcome = random_forest_game_outcomes(games_X_train, games_y_train, games_X_test, games_y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
